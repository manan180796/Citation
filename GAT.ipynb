{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GAT.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNla2toUrDflsGSvcvjc8+x",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/manan180796/Citation/blob/main/GAT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fDDKR7iGTLZK"
      },
      "source": [
        "!pip install spektral"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-JXK8-wTBQB"
      },
      "source": [
        "This example implements the experiments on citation networks from the paper:\n",
        "Graph Attention Networks (https://arxiv.org/abs/1710.10903)\n",
        "Petar Veličković, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Liò, Yoshua Bengio"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ugLQwvESr3h"
      },
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.layers import Input, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.regularizers import l2\n",
        "\n",
        "from spektral.datasets import citation\n",
        "from spektral.layers import GraphAttention"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nqbYdk_wTaAc"
      },
      "source": [
        "\n",
        "# Load data\n",
        "dataset = 'cora'\n",
        "A, X, y, train_mask, val_mask, test_mask = citation.load_data(dataset)\n",
        "\n",
        "# Parameters\n",
        "channels = 8            # Number of channel in each head of the first GAT layer\n",
        "n_attn_heads = 8        # Number of attention heads in first GAT layer\n",
        "N = X.shape[0]          # Number of nodes in the graph\n",
        "F = X.shape[1]          # Original size of node features\n",
        "n_classes = y.shape[1]  # Number of classes\n",
        "dropout = 0.6           # Dropout rate for the features and adjacency matrix\n",
        "l2_reg = 5e-6           # L2 regularization rate\n",
        "learning_rate = 5e-3    # Learning rate\n",
        "epochs = 20000          # Number of training epochs\n",
        "es_patience = 100       # Patience for early stopping"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RcNQbavIUC2U"
      },
      "source": [
        "# Model definition\n",
        "X_in = Input(shape=(F, ))\n",
        "A_in = Input(shape=(N, ), sparse=True)\n",
        "\n",
        "dropout_1 = Dropout(dropout)(X_in)\n",
        "graph_attention_1 = GraphAttention(channels,\n",
        "                                   attn_heads=n_attn_heads,\n",
        "                                   concat_heads=True,\n",
        "                                   dropout_rate=dropout,\n",
        "                                   activation='elu',\n",
        "                                   kernel_regularizer=l2(l2_reg),\n",
        "                                   attn_kernel_regularizer=l2(l2_reg)\n",
        "                                   )([dropout_1, A_in])\n",
        "dropout_2 = Dropout(dropout)(graph_attention_1)\n",
        "graph_attention_2 = GraphAttention(n_classes,\n",
        "                                   attn_heads=1,\n",
        "                                   concat_heads=False,\n",
        "                                   dropout_rate=dropout,\n",
        "                                   activation='softmax',\n",
        "                                   kernel_regularizer=l2(l2_reg),\n",
        "                                   attn_kernel_regularizer=l2(l2_reg)\n",
        "                                   )([dropout_2, A_in])\n",
        "# Build model\n",
        "model = Model(inputs=[X_in, A_in], outputs=graph_attention_2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p4K7gH7zUuTN"
      },
      "source": [
        "optimizer = Adam(lr=learning_rate)\n",
        "model.compile(optimizer=optimizer,\n",
        "              loss='categorical_crossentropy',\n",
        "              weighted_metrics=['acc'])\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ULNjGNJoZnoL"
      },
      "source": [
        "# Preprocessing operations\n",
        "A = A.astype('f4')\n",
        "X = X.toarray()\n",
        "validation_data = ([X, A], y, val_mask)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "swGIVzMbUygz"
      },
      "source": [
        "\n",
        "model.fit([X, A],\n",
        "          y,\n",
        "          sample_weight=train_mask,\n",
        "          epochs=epochs,\n",
        "          batch_size=N,\n",
        "          validation_data=validation_data,\n",
        "          shuffle=False,  # Shuffling data means shuffling the whole graph\n",
        "          callbacks=[\n",
        "              EarlyStopping(patience=es_patience, restore_best_weights=True)\n",
        "          ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U9_NyXd4U7U8"
      },
      "source": [
        "# Evaluate model\n",
        "print('Evaluating model.')\n",
        "eval_results = model.evaluate([X, A],\n",
        "                              y,\n",
        "                              sample_weight=test_mask,\n",
        "                              batch_size=N)\n",
        "print('Done.\\n'\n",
        "      'Test loss: {}\\n'\n",
        "      'Test accuracy: {}'.format(*eval_results))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}